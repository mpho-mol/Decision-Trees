Decision trees are a foundational algorithm in machine learning used for regression and classification problems. They are an illustration of a decision-making process and can be considered as a flowchart-like structure where each internal node represents a decision or a test on a specific feature, each branch represents the outcome of that decision or test, and each leaf node represents the final prediction. Decision trees are related to machine learning as they are a supervised learning algorithm and  provide a measure of feature importance where the more significant features are closer to the root node or used in more than one branch of the tree. Complex nonlinear relationships between features and the dependent variable can be observed thus versatile types of data can be modelled. Fine-tuning the hyperparameters of decision trees, like the minimum number of samples needed to split a node or maximum depth of the tree, cangreatly affect a model's performance. They serve as a building block for more advanced ensemble methods such as random forests.

The task was completed in Jupyter Notebook using the data in the titnic.csv file and the aim was to create a decsion tree predicting the survival of passengers on the Titanic. The 'Survived'(Y) variable was dropped from the DataFrame and the relevant features (X) were selected were then one-hot encoded. The data was split into training, development and test sets. A decision tree was trained without imposing restrictions and plotted. My model's accuracy on the development set was computed. Different values of the max_depth [2-10] were used to build the model, and at each step a plot of the tree was generated and the accuracies on the training and development data were stored. On the same graph, the lines of the training and development accuracies were plotted and their shapes were described. Commentary on the accuracy of the final model on the test data was made.
